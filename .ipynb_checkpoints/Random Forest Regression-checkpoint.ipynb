{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68fd242d-d8d7-4dfe-9662-7b609c3e71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset: (1795, 23)\n",
      "Mean Squared Error 0.21763180020024908\n",
      "Root Mean Squared Error 0.46651023590083107\n",
      "Adjusted R Squared Score:  0.0630743439083219\n",
      "Mean Absolute Error: 0.09057047630101249\n",
      "Mean Absolute Percentage Error: 12.35860801158546%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('cacao_engineered.csv')\n",
    "print(f\"shape of dataset: {df.shape}\")\n",
    "#split data 75:25\n",
    "x = df.loc[:, ~df.columns.isin(['Unnamed: 0', 'Rating'])]\n",
    "y = df['Rating']\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "#Random Forest regression with default values\n",
    "model_default = RandomForestRegressor()\n",
    "\n",
    "#fit model\n",
    "model_default.fit(x1_train, y1_train)\n",
    "\n",
    "#predict on model\n",
    "y1_pred_def = model_default.predict(x1_test)\n",
    "\n",
    "#evaluate model\n",
    "mse_def = mean_squared_error(y1_test, y1_pred_def)\n",
    "print(\"Mean Squared Error\", mse_def)\n",
    "\n",
    "rmse_def = mse_def**0.5\n",
    "print(\"Root Mean Squared Error\", rmse_def)\n",
    "\n",
    "r2score = r2_score(y1_test, y1_pred_def)\n",
    "#adjusted rscore is just rscore but the rscore can increase with additional features even though no improvement occurs for model's performance. \n",
    "#Basically adjusted negates the weakness of rscore\n",
    "adjusted_r2score_def = 1- ((1 - r2score) * ((df.shape[0] - 1) / (df.shape[0] - df.shape[1] - 1)))\n",
    "print(\"Adjusted R Squared Score: \", adjusted_r2score_def)\n",
    "\n",
    "mae_def = (1/ df.shape[0]) * sum(abs(y1_test - y1_pred_def))\n",
    "print(f\"Mean Absolute Error: {mae_def}\")\n",
    "errors = abs(y1_test - y1_pred_def)\n",
    "mape_def = (1/len(y1_test)) * sum(errors / y1_test) * 100\n",
    "print(f\"Mean Absolute Percentage Error: {mape_def}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4abc90-accf-4299-89a9-517cca06bd31",
   "metadata": {},
   "source": [
    "Above output is from the Random Forest Regression with the default settings. Currently, according to the Adjusted R Squared Score, the model is not performing well since it has a score close to 0. In other words, the features currently do not predict the ratings very well. The Mean Squared Error, Root Mean Squared Error, Mean Absolute Error, and Mean Absolute Percentage Error currently don't have much meaning since we don't have other measurements to compare them to. So let's try using grid search to see parameters work best for Random Forest Regression. Let's first see what range of parameter works best for our model using Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5de688e0-d6f9-44d8-9cab-028f8253b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "\n",
      "Best parameters from Random Search: {'n_estimators': 700, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_samples': None, 'max_features': 'sqrt', 'max_depth': 20}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters from Random Search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m rand_best_params \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msearch\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     32\u001b[0m model_random \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     33\u001b[0m y1_pred_rand \u001b[38;5;241m=\u001b[39m model_random\u001b[38;5;241m.\u001b[39mpredict(x1_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv('cacao_engineered.csv')\n",
    "# print(f\"shape of dataset: {df.shape}\")\n",
    "# #split data 75:25\n",
    "# x = df.loc[:, ~df.columns.isin(['Unnamed: 0', 'Rating'])]\n",
    "# y = df['Rating']\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "\n",
    "model_random = RandomForestRegressor()\n",
    "print(model_random.get_params())\n",
    "param_grid = {\n",
    "    'max_samples': [None, 5 ,20, 50 , 80, 100, 200],\n",
    "    'max_depth': [None, 10,20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'max_features': ['sqrt', 'log2', 1],\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'min_samples_split': [2,3,5, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4 , 5, 6, 7,8,9, 10]\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=model_random, param_distributions=param_grid, n_iter=200, cv=5, error_score='raise', verbose=3, n_jobs=-1)\n",
    "random_search.fit(x1_train, y1_train)\n",
    "print('\\n')\n",
    "print(f\"Best parameters from Random Search: {random_search.best_params_}\")\n",
    "model_random = random_search.best_estimator_\n",
    "y1_pred_rand = model_random.predict(x1_test)\n",
    "\n",
    "#evaluate model\n",
    "mse_rand = mean_squared_error(y1_test, y1_pred_rand)\n",
    "print(\"Mean Squared Error\", mse_rand)\n",
    "\n",
    "rmse_rand = mse_rand**0.5\n",
    "print(\"Root Mean Squared Error\", rmse_rand)\n",
    "\n",
    "r2score = r2_score(y1_test, y1_pred_rand)\n",
    "#adjusted rscore is just rscore but the rscore can increase with additional features even though no improvement occurs for model's performance. \n",
    "#Basically adjusted negates the weakness of rscore\n",
    "adjusted_r2score_rand = 1- ((1 - r2score) * ((df.shape[0] - 1) / (df.shape[0] - df.shape[1] - 1)))\n",
    "print(\"Adjusted R Squared Score: \", adjusted_r2score_rand)\n",
    "\n",
    "mae_rand = (1/ df.shape[0]) * sum(abs(y1_test - y1_pred_rand))\n",
    "print(f\"Mean Absolute Error: {mae_rand}\")\n",
    "errors = abs(y1_test - y1_pred_rand)\n",
    "mape_rand = (1/len(y1_test)) * sum(errors / y1_test) * 100\n",
    "print(f\"Mean Absolute Percentage Error: {mape_rand}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829f089-0c02-4518-a5ab-48bca6ada5d5",
   "metadata": {},
   "source": [
    "With the best parameters returned, we can use these values to explore nearby values using Grid Search. First, let's examine the performance evaluation for the model using these parameters. Comparing to the default mode, MSE, RMSE, and Adjsted R Square Score barely changed at all. So no reduction in error came from randomly searching for the best parameters and the Adjusted R Square score shows that the model still struggles to capture the data trend from the dataset features. MAPE didn't receive much change either. Let's see if we can gain more insights through Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625eed5f-20d7-49fe-860c-06201fa6c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# df = pd.read_csv('cacao_engineered.csv')\n",
    "# print(f\"shape of dataset: {df.shape}\")\n",
    "# #split data 75:25\n",
    "# x = df.loc[:, ~df.columns.isin(['Unnamed: 0', 'Rating'])]\n",
    "# y = df['Rating']\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "#Random Forest regression with gridsearch values\n",
    "model_grid = RandomForestRegressor()\n",
    "\n",
    "#trying a range of values close to the Random Search parameters\n",
    "#Best parameters from Random Search: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_samples': None, 'max_features': 'log2', 'max_depth': 20}\n",
    "param_grid = {\n",
    "    'max_samples': [None, 100, 500],\n",
    "    'max_depth': [None, 80, 90, 100],\n",
    "    'max_features': ['log2'],\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'min_samples_split': [4,5,6],\n",
    "    'min_samples_leaf': [2, 3, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = model_grid, param_grid = param_grid, verbose= 4, error_score='raise', n_jobs = -1)\n",
    "grid_result = grid_search.fit(x1_train, y1_train)\n",
    "print(f\"best params for the model are {grid_search.best_params_}\")\n",
    "\n",
    "#predict on model with best parameters from grid search\n",
    "model_grid = grid_search.best_estimator_\n",
    "y1_pred_grid = model_grid.predict(x1_test)\n",
    "#evaluate model\n",
    "mse_grid = mean_squared_error(y1_test, y1_pred_grid)\n",
    "print(\"Mean Squared Error\", mse_grid)\n",
    "rmse_grid = mse_grid**0.5\n",
    "print(\"Root Mean Squared Error\", rmse_grid)\n",
    "r2score = r2_score(y1_test, y1_pred_grid)\n",
    "#adjusted rscore is just rscore but the rscore only improves if additional features actually improves the model's performance. \n",
    "#Basically it negates the weakness of rscore which is that the score increases as the number of features increase\n",
    "adjusted_r2score_grid = 1- ((1 - r2score) * ((df.shape[0] - 1) / (df.shape[0] - df.shape[1] - 1)))\n",
    "print(\"Adjusted R Squared Score: \", adjusted_r2score_grid)\n",
    "\n",
    "mae_grid = (1/ df.shape[0]) * sum(abs(y1_test - y1_pred_grid))\n",
    "print(f\"Mean Absolute Error: {mae_grid}\")\n",
    "errors = abs(y1_test - y1_pred_grid)\n",
    "mape_grid = (1/len(y1_test)) * sum(errors / y1_test) * 100\n",
    "print(f\"Mean Absolute Percentage Error: {mape_grid}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfd14d-d29c-4c6d-8061-ca65a2db6be9",
   "metadata": {},
   "source": [
    "Even with Grid Search, the model isn't that much better. MSE, RMSE, Adjusted R Squared score, and MAPE still did not change much. MSE and RMSE increase slightly and Adjusted R Sqaured Score reduced slightly. This suggest a tiny worsening of the model. MAPE increases as well which makes sense because the error in other metrics increased. Anyways, it looks like the model doesn't improve much regardless of what parameter settings we used.  However, there is still something we can do. According to our default, Grid Search setting, and especially Random Search, our Adjusted R Squared score show that the features do not predict the variance in chocolate ratings well, further proven by the fact that Adjusted R Squared score penalizes the score when adding in a new feature. The low score may indicate that our dataset is poorly engineered for the model to train on. Considering there are 22 features and a majority of them are from One Hot Encoding, this might be the case. Let's take a look at the feature importance of the dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea607560-98b4-40a6-89cc-e8bf90a1bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# df = pd.read_csv('cacao_engineered.csv')\n",
    "# #split data 75:25\n",
    "# x = df.loc[:, ~df.columns.isin(['Unnamed: 0', 'Rating'])]\n",
    "# y = df['Rating']\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "#model with default settings\n",
    "\n",
    "# model_import = RandomForestRegressor()\n",
    "# model_import.fit(x_train, y_train)\n",
    "\n",
    "importants_def = list(model_default.feature_importances_)\n",
    "importants_rand = list(model_random.feature_importances_)\n",
    "importants_grid = list(model_grid.feature_importances_)\n",
    "important_features_def = []\n",
    "important_features_rand = []\n",
    "important_features_grid = []\n",
    "df_features = list(x.columns)\n",
    "#pair each feature with their importance values\n",
    "for feature, important in zip(df_features, importants_def):\n",
    "    important_features_def.append((feature, important))\n",
    "for feature,important in zip(df_features, importants_rand):\n",
    "    important_features_rand.append((feature, important))\n",
    "for feature,important in zip(df_features, importants_grid):\n",
    "    important_features_grid.append((feature, important))\n",
    "important_features_def = sorted(important_features_def, key = lambda tuple: tuple[1], reverse=True)\n",
    "important_features_rand = sorted(important_features_rand, key = lambda tuple: tuple[1], reverse=True)\n",
    "important_features_grid = sorted(important_features_grid, key = lambda tuple: tuple[1], reverse=True)\n",
    "\n",
    "important_list = [('Default', importants_def), ('Random Search', importants_rand), ('Grid Search', importants_grid)]\n",
    "#model_strs = ['Default', 'Random Search', 'Grid Search']\n",
    "# for tuple in important_features:\n",
    "#     print(f\"Feature: {tuple[0]:20}       Importance:{tuple[1]}\")\n",
    "\n",
    "#bar graph of the important features\n",
    "for pair in important_list:\n",
    "    x_values = list(range(len(pair[1])))\n",
    "    figure = plt.figure()\n",
    "    figure.set_figwidth(20)\n",
    "    plt.bar(x_values, pair[1], orientation='vertical') \n",
    "    plt.xticks(x_values, df_features, rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(f'{pair[0]} Model Feature Importances')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be0849-2787-4163-99b3-7f765df2fd5c",
   "metadata": {},
   "source": [
    "The Bar graph of feature importances shows the vast majority of features contribute little to accurately predicting a chocolate's rating. Only 2 features received an importance score above 0.1 and they're REF and Cocoa percent. The review date barely reaches above 0.1 in the Grid Search chart. Quick reminder, REF is a value that indicates how recent the chocolate entry entered the dataset. A higher REF number means more recent. Since REF and cocoa percent are the only two features that demonstrate a great influence on model prediction. Let's see if we can get better results with just these features. Let's repeat our process by doing Random Search again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda9866-c83c-413a-bbfb-0d5621080d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('cacao_engineered.csv')\n",
    "#split data 75:25\n",
    "x = df.loc[:, df.columns.isin(['REF', 'Cocoa Percent'])]\n",
    "y = df['Rating']\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "\n",
    "model_random2 = RandomForestRegressor()\n",
    "param_grid = {\n",
    "    'max_samples': [None, 5 ,20, 50 , 80, 100, 200],\n",
    "    'max_depth': [None, 10,20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'max_features': ['sqrt', 'log2', 1],\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'min_samples_split': [2,3,5, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4 , 5, 6, 7,8,9, 10]\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=model_random2, param_distributions=param_grid, n_iter=200, cv=5, error_score='raise', verbose=3, n_jobs=-1)\n",
    "random_search.fit(x2_train, y2_train)\n",
    "model_random2 = random_search.best_estimator_\n",
    "model_random2.fit(x2_train, y2_train)\n",
    "y2_pred_rand2 = model_random2.predict(x2_test)\n",
    "print('\\n')\n",
    "print(f\"Best parameters from Random Search: {random_search.best_params_}\")\n",
    "\n",
    "#evaluate model\n",
    "mse_rand2 = mean_squared_error(y2_test, y2_pred_rand2)\n",
    "print(\"Mean Squared Error\", mse_rand2)\n",
    "\n",
    "rmse_rand2 = mse_rand2**0.5\n",
    "print(\"Root Mean Squared Error\", rmse_rand2)\n",
    "\n",
    "r2score = r2_score(y2_test, y2_pred_rand2)\n",
    "#adjusted rscore is just rscore but the rscore can increase with additional features even though no improvement occurs for model's performance. \n",
    "#Basically adjusted negates the weakness of rscore\n",
    "adjusted_r2score_rand2 = 1- ((1 - r2score) * ((df.shape[0] - 1) / (df.shape[0] - df.shape[1] - 1)))\n",
    "print(\"Adjusted R Squared Score: \", adjusted_r2score_rand2)\n",
    "\n",
    "mae_rand2 = (1/ df.shape[0]) * sum(abs(y2_test - y2_pred_rand2))\n",
    "print(f\"Mean Absolute Error: {mae_rand2}\")\n",
    "errors = abs(y2_test - y2_pred_rand2)\n",
    "mape_rand2 = (1/len(y2_test)) * sum(errors / y2_test) * 100\n",
    "print(f\"Mean Absolute Percentage Error: {mape_rand2}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46826671-9335-4958-97fa-d2a588830a80",
   "metadata": {},
   "source": [
    "Approximately the same values as the first Random Search and Grid Search. Adjusted R Squared score had decreased by half though.  Let's use these parameter values to perform Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725494f-72be-47da-89d6-6d8334dd271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# df = pd.read_csv('cacao_engineered.csv')\n",
    "# print(f\"shape of dataset: {df.shape}\")\n",
    "# #split data 75:25\n",
    "# x = df.loc[:, df.columns.isin(['REF', 'Cocoa Percent'])]\n",
    "# y = df['Rating']\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)\n",
    "\n",
    "#Random Forest regression with gridsearch values\n",
    "model_grid2 = RandomForestRegressor()\n",
    "\n",
    "#trying a range of values close to the Random Search parameters\n",
    "#Best parameters from Random Search: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_samples': 200, 'max_features': 'log2', 'max_depth': 90}\n",
    "param_grid = {\n",
    "    'max_samples': [None, 200, 300, 100],\n",
    "    'max_depth': [None, 80, 90, 100],\n",
    "    'max_features': ['sqrt'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_samples_split': [4,5,6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = model_grid2, param_grid = param_grid, verbose= 4, error_score='raise', n_jobs = -1)\n",
    "grid_result = grid_search.fit(x2_train, y2_train)\n",
    "print(f\"best params for the model are {grid_search.best_params_}\")\n",
    "\n",
    "#predict on model with best parameters from grid search\n",
    "model_grid2= grid_search.best_estimator_\n",
    "y2_pred_grid2 = model_grid2.predict(x2_test)\n",
    "#evaluate model\n",
    "mse_grid2 = mean_squared_error(y2_test, y2_pred_grid2)\n",
    "print(\"Mean Squared Error\", mse_grid2)\n",
    "rmse_grid2 = mse_grid2**0.5\n",
    "print(\"Root Mean Squared Error\", rmse_grid2)\n",
    "r2score = r2_score(y2_test, y2_pred_grid2)\n",
    "#adjusted rscore is just rscore but the rscore only improves if additional features actually improves the model's performance. \n",
    "#Basically it negates the weakness of rscore which is that the score increases as the number of features increase\n",
    "adjusted_r2score_grid2 = 1- ((1 - r2score) * ((df.shape[0] - 1) / (df.shape[0] - df.shape[1] - 1)))\n",
    "print(\"Adjusted R Squared Score: \", adjusted_r2score_grid2)\n",
    "\n",
    "mae_grid2 = (1/ df.shape[0]) * sum(abs(y2_test - y2_pred_grid2))\n",
    "print(f\"Mean Absolute Error: {mae_grid2}\")\n",
    "errors = abs(y2_test - y2_pred_grid2)\n",
    "mape_grid2 = (1/len(y2_test)) * sum(errors / y2_test) * 100\n",
    "print(f\"Mean Absolute Percentage Error: {mape_grid2}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af6681-7a5b-4040-b877-6ba1b5fd6d81",
   "metadata": {},
   "source": [
    "The measurements here don't look like that have improved much. Thus, it seems that Random Forest Regression struggles to predict on the dataset regardless of whether the geographical features are included or not. Let's plot the data we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d00b1-ee5f-437e-bfe1-48b0e99f58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# df = pd.read_csv('cacao_engineered.csv')\n",
    "# print(f\"shape of dataset: {df.shape}\")\n",
    "# #split data 75:25\n",
    "# x = df.loc[:, ~df.columns.isin(['Unnamed: 0', 'Rating'])]\n",
    "# y = df['Rating']\n",
    "\n",
    "# our models are model_default, model_random, model_grid, model_random2, model_grid2\n",
    "model_list = ['Default', 'Random Search', 'Grid Search', 'Random Search 2', 'Grid Search 2']\n",
    "metric_strs = [\"MSE\", \"RMSE\", \"Adjusted r^2 score\", \"MAE\", \"MAPE\"]\n",
    "mse_list = [mse_def, mse_rand, mse_grid, mse_rand2, mse_grid2]\n",
    "rmse_list = [rmse_def, rmse_rand, rmse_grid, rmse_rand2, rmse_grid2]\n",
    "adjusted_r2score_list = [adjusted_r2score_def, adjusted_r2score_rand, adjusted_r2score_grid, adjusted_r2score_rand2, adjusted_r2score_grid2]\n",
    "mae_list = [mae_def, mae_rand, mae_grid, mae_rand2, mae_grid2]\n",
    "acc_list = [mape_def, mape_rand, mape_grid, mape_rand2, mape_grid2]\n",
    "metric_list = [mse_list, rmse_list, adjusted_r2score_list, mae_list, acc_list]\n",
    "x_values = list(range(len(model_list)))\n",
    "\n",
    "for index, metric in enumerate(metric_list):\n",
    "    #bar chart\n",
    "    figure = plt.figure()\n",
    "    figure.set_figwidth(10)\n",
    "    plt.bar(x_values, metric, orientation='vertical')\n",
    "    plt.xticks(x_values, model_list, rotation=90)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(f\"{metric_strs[index]}\")\n",
    "    plt.title(f\"Model {metric_strs[index]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad938f5d-2bad-4d58-b177-e12037642500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "entry_num1 = list(range(len(x1_test)))\n",
    "entry_num2 = list(range(len(x2_test)))\n",
    "\n",
    "#plot just the actual values\n",
    "plt.plot(entry_num1, y1_test, '*', label='actual')\n",
    "plt.legend()\n",
    "plt.title('Actual values')\n",
    "plt.show()\n",
    "#plot actual values\n",
    "# figure = plt.figure()\n",
    "# figure.set_figwidth(10)\n",
    "# figure.set_figwidth(10)\n",
    "plt.plot(entry_num1, y1_test, '*', label='actual')\n",
    "#plot predicted values\n",
    "plt.plot(entry_num1, y1_pred_def, 'og', label='default predictions')\n",
    "plt.plot(entry_num1, y1_pred_rand, 'or', label='Random Search predictions')\n",
    "plt.plot(entry_num1, y1_pred_grid, 'oc', label='Grid Search predictions')\n",
    "plt.legend()\n",
    "plt.title('Actual and Predicted values')\n",
    "plt.show()\n",
    "\n",
    "#plot actual values\n",
    "plt.plot(entry_num2, y2_test, '*', label='actual')\n",
    "#plot predicted values\n",
    "plt.plot(entry_num2, y2_pred_rand2, 'om', label='Random Search 2 predictions')\n",
    "plt.plot(entry_num2, y2_pred_grid2, 'oy', label='Grid Search 2 predictions')\n",
    "plt.legend()\n",
    "plt.title('Actual and Predicted values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fb3a4-bf4b-4366-b6fd-c5bedd6b4619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c07037-8966-495f-ac8a-d774bef731ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
